aiAssistantLicense:
  activationKey: ""
  externalSecret:
    name: ""
    key: AI_ASSISTANT_ACTIVATION_KEY

apiAuth:
  apiToken: secret
  jwt:
    publicKey: none
    algorithm: RS256

sidecars:
  - name: openai-mocker
    image: python:3.11-alpine
    imagePullPolicy: IfNotPresent
    command:
      - /bin/sh
      - -ce
      - |-
        cat > /tmp/openai-compat-mock.py << 'EOF'
        import time, random
        import uvicorn
        from fastapi import FastAPI
        from pydantic import BaseModel
        from typing import List, Union, Optional

        app = FastAPI()

        # Embeddings (OpenAI-compatible)
        class EmbeddingRequest(BaseModel):
            input: Union[str, List[str]]
            model: Optional[str] = "text-embedding-3-small"

        @app.post("/v1/embeddings")
        async def create_embeddings(request: EmbeddingRequest):
            inputs = request.input if isinstance(request.input, list) else [request.input]
            embeddings = []
            for i, text in enumerate(inputs):
                embedding = [random.uniform(-1, 1) for _ in range(1536)]
                embeddings.append({
                    "object": "embedding",
                    "index": i,
                    "embedding": embedding
                })
            total_tokens = sum(len((text or "").split()) for text in inputs)
            return {
                "object": "list",
                "data": embeddings,
                "model": request.model,
                "usage": {
                    "prompt_tokens": total_tokens,
                    "total_tokens": total_tokens
                }
            }

        # Chat Completions (OpenAI-compatible)
        class ChatMessage(BaseModel):
            role: str
            content: Union[str, List[dict]]

        class ChatRequest(BaseModel):
            model: Optional[str] = "gpt-4o-mini"
            messages: List[ChatMessage]

        @app.post("/v1/chat/completions")
        async def chat_completions(req: ChatRequest):
            now = int(time.time())
            reply = "Hello from the local mock chat model."
            return {
                "id": f"chatcmpl-{now}",
                "object": "chat.completion",
                "created": now,
                "model": req.model,
                "choices": [{
                    "index": 0,
                    "message": {"role": "assistant", "content": reply},
                    "finish_reason": "stop"
                }],
                "usage": {"prompt_tokens": 1, "completion_tokens": 3, "total_tokens": 4}
            }

        if __name__ == "__main__":
            uvicorn.run(app, host="0.0.0.0", port=8080)
        EOF

        pip install fastapi uvicorn && python /tmp/openai-compat-mock.py

config:
  serviceConfiguration:
    version: '1'
    aiServices:
      chat:
        provider:
          name: "openai-compat"
          baseUrl: "http://localhost:8080/v1"
        model: "gpt-4"
      textEmbeddings:
        provider:
          name: "openai-compat"
          baseUrl: "http://localhost:8080/v1"
        model: "text-embedding-3-small"
  aiServiceProviderCredentials:
    openAICompatible:
      apiKey: "dummy-token"
  documentEngine:
    enabled: true
    url: http://document-engine:5000
    auth:
      apiToken: documentEngineSecret

database:
  enabled: true
  engine: postgres
  postgres:
    username: postgres
    password: nutrientArtificialIntelligenceAssistant
    tls:
      enabled: false

dashboard:
  auth:
    username: admin
    password: admin

observability:
  log:
    level: info
    socketTraces: false
  opentelemetry:
    enabled: false

replicaCount: 1

updateStrategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 1
    maxSurge: 0

resources:
  requests:
    cpu: "100m"
    memory: 256Mi
  limits:
    cpu: "2"
    memory: 4Gi

ingress:
  enabled: true
  className: nginx
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/proxy-body-size: "0"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "180"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "180"
    nginx.ingress.kubernetes.io/large-client-header-buffers: "4 16k"
    nginx.ingress.kubernetes.io/proxy-buffer-size: "128k"
  hosts:
    - paths:
        - path: /
          pathType: ImplementationSpecific

cloudNativePG:
  enabled: true
  clusterName: "{{ .Release.Name }}-postgres"
  clusterSpec:
    logLevel: info
  documentEngineDatabase:
    enabled: true
    name: document_engine

document-engine:
  enabled: true
  fullnameOverride: document-engine
  nameOverride: document-engine
  documentEngineLicense:
    activationKey: ""
    externalSecret:
      name: ""
  apiAuth:
    apiToken: documentEngineSecret
  config:
    trustedProxies: default
  documentSigningService:
    enabled: false
  database:
    enabled: true
    engine: postgres
    migrationJob:
      enabled: false
    postgres:
      host: "{{ .Release.Name }}-de-postgres-rw"
      database: document_engine
      # username: postgres
      # password: nutrientArtificialIntelligenceAssistant
      # adminUsername: postgres
      # adminPassword: nutrientArtificialIntelligenceAssistant
      tls:
        enabled: false
  assetStorage:
    backendType: built-in
    backendFallback:
      enabled: false
  dashboard:
    auth:
      username: admin
      password: admin
  observability:
    log:
      level: info
    opentelemetry:
      enabled: false
  replicaCount: 1
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 0
  resources:
    requests:
      cpu: "100m"
      memory: 256Mi
    limits:
      cpu: "2"
      memory: 4Gi
  ingress:
    enabled: true
    className: nginx
    annotations:
      nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
      nginx.ingress.kubernetes.io/rewrite-target: /
      nginx.ingress.kubernetes.io/proxy-body-size: "0"
      nginx.ingress.kubernetes.io/proxy-send-timeout: "180"
      nginx.ingress.kubernetes.io/proxy-read-timeout: "180"
      nginx.ingress.kubernetes.io/large-client-header-buffers: "4 16k"
      nginx.ingress.kubernetes.io/proxy-buffer-size: "128k"
    hosts:
      - paths:
          - path: /
            pathType: ImplementationSpecific
  startupProbe:
    initialDelaySeconds: 30
    failureThreshold: 10
  cloudNativePG:
    enabled: true
    clusterName: "{{ .Release.Name }}-de-postgres"
    clusterSpec:
      logLevel: info
